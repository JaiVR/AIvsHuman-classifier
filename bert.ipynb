{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d183a54c-1b20-4513-9aad-09ed327d88d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jai54\\anaconda3\\envs\\cuda_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jai54\\anaconda3\\envs\\cuda_env\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import DefaultDataCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b031db9a-d679-40bf-b297-a4ca97262f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"csv\", data_files=\"AI_Human.csv\")\n",
    "dataset = dataset[\"train\"]\n",
    "\n",
    "dataset = dataset.shuffle(seed=42).select(range(20000))\n",
    "\n",
    "split_dataset = dataset.train_test_split(test_size=0.2)\n",
    "test_valid = split_dataset[\"test\"].train_test_split(test_size=0.5)\n",
    "\n",
    "final_splits = {\n",
    "    \"train\": split_dataset[\"train\"],\n",
    "    \"validation\": test_valid[\"train\"],\n",
    "    \"test\": test_valid[\"test\"],\n",
    "}\n",
    "\n",
    "dataset_dict = final_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c7f737f-f210-4d8c-88ee-8bdbca2ebef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['text', 'generated'],\n",
       "     num_rows: 16000\n",
       " }),\n",
       " 'validation': Dataset({\n",
       "     features: ['text', 'generated'],\n",
       "     num_rows: 2000\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['text', 'generated'],\n",
       "     num_rows: 2000\n",
       " })}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73ade83f-8c22-45ef-bd41-50df9126322e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "dataset_dict = DatasetDict(dataset_dict)  # Convert it to DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "140bfcd3-34b3-4919-a4c0-55ba7e943c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = dataset_dict.rename_column(\"generated\", \"labels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ed951c6-e613-4e89-a920-3a85c677ae91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'labels': Value(dtype='float64', id=None)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36c9bf5f-1c22-4ad5-917f-8a485b4d0f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61dea22c-f966-424d-94da-5378c293af18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 16000/16000 [00:00<00:00, 21202.11 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 23200.51 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 21994.94 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def convert_labels(example):\n",
    "    example[\"labels\"] = int(example[\"labels\"])\n",
    "    return example\n",
    "\n",
    "dataset_dict = dataset_dict.map(convert_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d21d9977-a82c-4077-9de8-126a318254c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0.0: 10004, 1.0: 5996})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(Counter(dataset_dict[\"train\"][\"labels\"]))  # Count occurrences of each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e761fb0-ac02-47ad-b1eb-f22439410dc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(dataset_dict[\"train\"][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c793174f-826c-45fa-8518-baf5aa422d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(dataset_dict[\"train\"][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a0f2a44-1b50-4fd0-8056-2e3472650187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Is it possible for younger and older students to work together? Older students are smarter than younger students, because they have more experience than the younger students. Also, the older students could help the younger students with their school work. Schools should have a program that Paris older students with younger students because they could help each other with school works, help them learn more about team work, and they could learn some viable things and give each other great advice when they need it.\\n\\nHow could older students help younger students with school work. Older students are smarter than younger students, whereas younger students could learn faster than older students. My older sister is really smart every time I ask for help she always helps me, and she shows me how to solve the problems that I don't understand. After she shows me how to solve the problem I would understand the question and solve the other problems. That is how older students could help younger students with their work.\\n\\nOlder and younger students learning about team work together. Team work is something really important to learn about, because it could help you talk with more people, and you could get great ideas from other people. When older and younger students work as a team it could benefit them both with their team work and their project that they are working on together. One time I had to work with an older student with this project that I had to do. They were such a great help, and they showed me how to start it and how to end it. They were so intelligent, and they would tell me if I had done something wrong and how I should correct it. That is how older and younger students could learn more about team work together.\\n\\nOlder and younger students learning viable things and give each other great advice when one needs it. When I was in third grade I was going to have my first SOL, I was so nervous and scared about it. But then my older brother gave me great advice and that I should calm down and focus on it. He helped me to study for, and he was a huge help. That is why older students can give you perfect advice when you need it, and younger students too can give great advice. Therefor, older and younger students can teach each other viable things and give good advice to each other.\\n\\nSchools should have a program that pairs older students with younger students, because they could give each other great advice and learn viable things from each other, And they could help each other with team work.\\n\\nAlso, they could help each other with school work. Older students are sometimes wise when it comes to helping younger students, in other words they give great advice. Team work is something that we all need to learn about because It's important to work with other people and hear their ideas. When you work with older students they will help you understand how to do the project, or it could be the other way around it depends on the work that you are going to get, or it depends on the students that you are working with. Older and younger students sometimes team up together for school work,\\n\\nAnd they always help each other with it. Therefor, schools should have a program that Paris up older students with younger students.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict[\"train\"][\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c211ec5b-1ebf-498b-b28f-f8315a41c1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"google-bert/bert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "id2label = {0: \"Human\", 1: \"AI\"}\n",
    "label2id = {\"Human\": 0, \"AI\": 1}\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=2, id2label=id2label, label2id=label2id)\n",
    "model.config.problem_type = \"single_label_classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "132b54ea-536b-4db5-a88d-0cb93bc10b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight True\n",
      "bert.embeddings.position_embeddings.weight True\n",
      "bert.embeddings.token_type_embeddings.weight True\n",
      "bert.embeddings.LayerNorm.weight True\n",
      "bert.embeddings.LayerNorm.bias True\n",
      "bert.encoder.layer.0.attention.self.query.weight True\n",
      "bert.encoder.layer.0.attention.self.query.bias True\n",
      "bert.encoder.layer.0.attention.self.key.weight True\n",
      "bert.encoder.layer.0.attention.self.key.bias True\n",
      "bert.encoder.layer.0.attention.self.value.weight True\n",
      "bert.encoder.layer.0.attention.self.value.bias True\n",
      "bert.encoder.layer.0.attention.output.dense.weight True\n",
      "bert.encoder.layer.0.attention.output.dense.bias True\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.0.intermediate.dense.weight True\n",
      "bert.encoder.layer.0.intermediate.dense.bias True\n",
      "bert.encoder.layer.0.output.dense.weight True\n",
      "bert.encoder.layer.0.output.dense.bias True\n",
      "bert.encoder.layer.0.output.LayerNorm.weight True\n",
      "bert.encoder.layer.0.output.LayerNorm.bias True\n",
      "bert.encoder.layer.1.attention.self.query.weight True\n",
      "bert.encoder.layer.1.attention.self.query.bias True\n",
      "bert.encoder.layer.1.attention.self.key.weight True\n",
      "bert.encoder.layer.1.attention.self.key.bias True\n",
      "bert.encoder.layer.1.attention.self.value.weight True\n",
      "bert.encoder.layer.1.attention.self.value.bias True\n",
      "bert.encoder.layer.1.attention.output.dense.weight True\n",
      "bert.encoder.layer.1.attention.output.dense.bias True\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.1.intermediate.dense.weight True\n",
      "bert.encoder.layer.1.intermediate.dense.bias True\n",
      "bert.encoder.layer.1.output.dense.weight True\n",
      "bert.encoder.layer.1.output.dense.bias True\n",
      "bert.encoder.layer.1.output.LayerNorm.weight True\n",
      "bert.encoder.layer.1.output.LayerNorm.bias True\n",
      "bert.encoder.layer.2.attention.self.query.weight True\n",
      "bert.encoder.layer.2.attention.self.query.bias True\n",
      "bert.encoder.layer.2.attention.self.key.weight True\n",
      "bert.encoder.layer.2.attention.self.key.bias True\n",
      "bert.encoder.layer.2.attention.self.value.weight True\n",
      "bert.encoder.layer.2.attention.self.value.bias True\n",
      "bert.encoder.layer.2.attention.output.dense.weight True\n",
      "bert.encoder.layer.2.attention.output.dense.bias True\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.2.intermediate.dense.weight True\n",
      "bert.encoder.layer.2.intermediate.dense.bias True\n",
      "bert.encoder.layer.2.output.dense.weight True\n",
      "bert.encoder.layer.2.output.dense.bias True\n",
      "bert.encoder.layer.2.output.LayerNorm.weight True\n",
      "bert.encoder.layer.2.output.LayerNorm.bias True\n",
      "bert.encoder.layer.3.attention.self.query.weight True\n",
      "bert.encoder.layer.3.attention.self.query.bias True\n",
      "bert.encoder.layer.3.attention.self.key.weight True\n",
      "bert.encoder.layer.3.attention.self.key.bias True\n",
      "bert.encoder.layer.3.attention.self.value.weight True\n",
      "bert.encoder.layer.3.attention.self.value.bias True\n",
      "bert.encoder.layer.3.attention.output.dense.weight True\n",
      "bert.encoder.layer.3.attention.output.dense.bias True\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.3.intermediate.dense.weight True\n",
      "bert.encoder.layer.3.intermediate.dense.bias True\n",
      "bert.encoder.layer.3.output.dense.weight True\n",
      "bert.encoder.layer.3.output.dense.bias True\n",
      "bert.encoder.layer.3.output.LayerNorm.weight True\n",
      "bert.encoder.layer.3.output.LayerNorm.bias True\n",
      "bert.encoder.layer.4.attention.self.query.weight True\n",
      "bert.encoder.layer.4.attention.self.query.bias True\n",
      "bert.encoder.layer.4.attention.self.key.weight True\n",
      "bert.encoder.layer.4.attention.self.key.bias True\n",
      "bert.encoder.layer.4.attention.self.value.weight True\n",
      "bert.encoder.layer.4.attention.self.value.bias True\n",
      "bert.encoder.layer.4.attention.output.dense.weight True\n",
      "bert.encoder.layer.4.attention.output.dense.bias True\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.4.intermediate.dense.weight True\n",
      "bert.encoder.layer.4.intermediate.dense.bias True\n",
      "bert.encoder.layer.4.output.dense.weight True\n",
      "bert.encoder.layer.4.output.dense.bias True\n",
      "bert.encoder.layer.4.output.LayerNorm.weight True\n",
      "bert.encoder.layer.4.output.LayerNorm.bias True\n",
      "bert.encoder.layer.5.attention.self.query.weight True\n",
      "bert.encoder.layer.5.attention.self.query.bias True\n",
      "bert.encoder.layer.5.attention.self.key.weight True\n",
      "bert.encoder.layer.5.attention.self.key.bias True\n",
      "bert.encoder.layer.5.attention.self.value.weight True\n",
      "bert.encoder.layer.5.attention.self.value.bias True\n",
      "bert.encoder.layer.5.attention.output.dense.weight True\n",
      "bert.encoder.layer.5.attention.output.dense.bias True\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.5.intermediate.dense.weight True\n",
      "bert.encoder.layer.5.intermediate.dense.bias True\n",
      "bert.encoder.layer.5.output.dense.weight True\n",
      "bert.encoder.layer.5.output.dense.bias True\n",
      "bert.encoder.layer.5.output.LayerNorm.weight True\n",
      "bert.encoder.layer.5.output.LayerNorm.bias True\n",
      "bert.encoder.layer.6.attention.self.query.weight True\n",
      "bert.encoder.layer.6.attention.self.query.bias True\n",
      "bert.encoder.layer.6.attention.self.key.weight True\n",
      "bert.encoder.layer.6.attention.self.key.bias True\n",
      "bert.encoder.layer.6.attention.self.value.weight True\n",
      "bert.encoder.layer.6.attention.self.value.bias True\n",
      "bert.encoder.layer.6.attention.output.dense.weight True\n",
      "bert.encoder.layer.6.attention.output.dense.bias True\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.6.intermediate.dense.weight True\n",
      "bert.encoder.layer.6.intermediate.dense.bias True\n",
      "bert.encoder.layer.6.output.dense.weight True\n",
      "bert.encoder.layer.6.output.dense.bias True\n",
      "bert.encoder.layer.6.output.LayerNorm.weight True\n",
      "bert.encoder.layer.6.output.LayerNorm.bias True\n",
      "bert.encoder.layer.7.attention.self.query.weight True\n",
      "bert.encoder.layer.7.attention.self.query.bias True\n",
      "bert.encoder.layer.7.attention.self.key.weight True\n",
      "bert.encoder.layer.7.attention.self.key.bias True\n",
      "bert.encoder.layer.7.attention.self.value.weight True\n",
      "bert.encoder.layer.7.attention.self.value.bias True\n",
      "bert.encoder.layer.7.attention.output.dense.weight True\n",
      "bert.encoder.layer.7.attention.output.dense.bias True\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.7.intermediate.dense.weight True\n",
      "bert.encoder.layer.7.intermediate.dense.bias True\n",
      "bert.encoder.layer.7.output.dense.weight True\n",
      "bert.encoder.layer.7.output.dense.bias True\n",
      "bert.encoder.layer.7.output.LayerNorm.weight True\n",
      "bert.encoder.layer.7.output.LayerNorm.bias True\n",
      "bert.encoder.layer.8.attention.self.query.weight True\n",
      "bert.encoder.layer.8.attention.self.query.bias True\n",
      "bert.encoder.layer.8.attention.self.key.weight True\n",
      "bert.encoder.layer.8.attention.self.key.bias True\n",
      "bert.encoder.layer.8.attention.self.value.weight True\n",
      "bert.encoder.layer.8.attention.self.value.bias True\n",
      "bert.encoder.layer.8.attention.output.dense.weight True\n",
      "bert.encoder.layer.8.attention.output.dense.bias True\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.8.intermediate.dense.weight True\n",
      "bert.encoder.layer.8.intermediate.dense.bias True\n",
      "bert.encoder.layer.8.output.dense.weight True\n",
      "bert.encoder.layer.8.output.dense.bias True\n",
      "bert.encoder.layer.8.output.LayerNorm.weight True\n",
      "bert.encoder.layer.8.output.LayerNorm.bias True\n",
      "bert.encoder.layer.9.attention.self.query.weight True\n",
      "bert.encoder.layer.9.attention.self.query.bias True\n",
      "bert.encoder.layer.9.attention.self.key.weight True\n",
      "bert.encoder.layer.9.attention.self.key.bias True\n",
      "bert.encoder.layer.9.attention.self.value.weight True\n",
      "bert.encoder.layer.9.attention.self.value.bias True\n",
      "bert.encoder.layer.9.attention.output.dense.weight True\n",
      "bert.encoder.layer.9.attention.output.dense.bias True\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.9.intermediate.dense.weight True\n",
      "bert.encoder.layer.9.intermediate.dense.bias True\n",
      "bert.encoder.layer.9.output.dense.weight True\n",
      "bert.encoder.layer.9.output.dense.bias True\n",
      "bert.encoder.layer.9.output.LayerNorm.weight True\n",
      "bert.encoder.layer.9.output.LayerNorm.bias True\n",
      "bert.encoder.layer.10.attention.self.query.weight True\n",
      "bert.encoder.layer.10.attention.self.query.bias True\n",
      "bert.encoder.layer.10.attention.self.key.weight True\n",
      "bert.encoder.layer.10.attention.self.key.bias True\n",
      "bert.encoder.layer.10.attention.self.value.weight True\n",
      "bert.encoder.layer.10.attention.self.value.bias True\n",
      "bert.encoder.layer.10.attention.output.dense.weight True\n",
      "bert.encoder.layer.10.attention.output.dense.bias True\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.10.intermediate.dense.weight True\n",
      "bert.encoder.layer.10.intermediate.dense.bias True\n",
      "bert.encoder.layer.10.output.dense.weight True\n",
      "bert.encoder.layer.10.output.dense.bias True\n",
      "bert.encoder.layer.10.output.LayerNorm.weight True\n",
      "bert.encoder.layer.10.output.LayerNorm.bias True\n",
      "bert.encoder.layer.11.attention.self.query.weight True\n",
      "bert.encoder.layer.11.attention.self.query.bias True\n",
      "bert.encoder.layer.11.attention.self.key.weight True\n",
      "bert.encoder.layer.11.attention.self.key.bias True\n",
      "bert.encoder.layer.11.attention.self.value.weight True\n",
      "bert.encoder.layer.11.attention.self.value.bias True\n",
      "bert.encoder.layer.11.attention.output.dense.weight True\n",
      "bert.encoder.layer.11.attention.output.dense.bias True\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.11.intermediate.dense.weight True\n",
      "bert.encoder.layer.11.intermediate.dense.bias True\n",
      "bert.encoder.layer.11.output.dense.weight True\n",
      "bert.encoder.layer.11.output.dense.bias True\n",
      "bert.encoder.layer.11.output.LayerNorm.weight True\n",
      "bert.encoder.layer.11.output.LayerNorm.bias True\n",
      "bert.pooler.dense.weight True\n",
      "bert.pooler.dense.bias True\n",
      "classifier.weight True\n",
      "classifier.bias True\n"
     ]
    }
   ],
   "source": [
    "#layers\n",
    "for name, param in model.named_parameters():\n",
    "   print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd6b86f4-518c-4321-ab28-27afe3813c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze base model params\n",
    "for name, param in model.base_model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# unfreeze base model pooling layers\n",
    "for name, param in model.base_model.named_parameters():\n",
    "    if \"pooler\" in name:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1f3a3f8-4396-40e6-bfbc-1b2b70ad7ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight False\n",
      "bert.embeddings.position_embeddings.weight False\n",
      "bert.embeddings.token_type_embeddings.weight False\n",
      "bert.embeddings.LayerNorm.weight False\n",
      "bert.embeddings.LayerNorm.bias False\n",
      "bert.encoder.layer.0.attention.self.query.weight False\n",
      "bert.encoder.layer.0.attention.self.query.bias False\n",
      "bert.encoder.layer.0.attention.self.key.weight False\n",
      "bert.encoder.layer.0.attention.self.key.bias False\n",
      "bert.encoder.layer.0.attention.self.value.weight False\n",
      "bert.encoder.layer.0.attention.self.value.bias False\n",
      "bert.encoder.layer.0.attention.output.dense.weight False\n",
      "bert.encoder.layer.0.attention.output.dense.bias False\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.0.intermediate.dense.weight False\n",
      "bert.encoder.layer.0.intermediate.dense.bias False\n",
      "bert.encoder.layer.0.output.dense.weight False\n",
      "bert.encoder.layer.0.output.dense.bias False\n",
      "bert.encoder.layer.0.output.LayerNorm.weight False\n",
      "bert.encoder.layer.0.output.LayerNorm.bias False\n",
      "bert.encoder.layer.1.attention.self.query.weight False\n",
      "bert.encoder.layer.1.attention.self.query.bias False\n",
      "bert.encoder.layer.1.attention.self.key.weight False\n",
      "bert.encoder.layer.1.attention.self.key.bias False\n",
      "bert.encoder.layer.1.attention.self.value.weight False\n",
      "bert.encoder.layer.1.attention.self.value.bias False\n",
      "bert.encoder.layer.1.attention.output.dense.weight False\n",
      "bert.encoder.layer.1.attention.output.dense.bias False\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.1.intermediate.dense.weight False\n",
      "bert.encoder.layer.1.intermediate.dense.bias False\n",
      "bert.encoder.layer.1.output.dense.weight False\n",
      "bert.encoder.layer.1.output.dense.bias False\n",
      "bert.encoder.layer.1.output.LayerNorm.weight False\n",
      "bert.encoder.layer.1.output.LayerNorm.bias False\n",
      "bert.encoder.layer.2.attention.self.query.weight False\n",
      "bert.encoder.layer.2.attention.self.query.bias False\n",
      "bert.encoder.layer.2.attention.self.key.weight False\n",
      "bert.encoder.layer.2.attention.self.key.bias False\n",
      "bert.encoder.layer.2.attention.self.value.weight False\n",
      "bert.encoder.layer.2.attention.self.value.bias False\n",
      "bert.encoder.layer.2.attention.output.dense.weight False\n",
      "bert.encoder.layer.2.attention.output.dense.bias False\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.2.intermediate.dense.weight False\n",
      "bert.encoder.layer.2.intermediate.dense.bias False\n",
      "bert.encoder.layer.2.output.dense.weight False\n",
      "bert.encoder.layer.2.output.dense.bias False\n",
      "bert.encoder.layer.2.output.LayerNorm.weight False\n",
      "bert.encoder.layer.2.output.LayerNorm.bias False\n",
      "bert.encoder.layer.3.attention.self.query.weight False\n",
      "bert.encoder.layer.3.attention.self.query.bias False\n",
      "bert.encoder.layer.3.attention.self.key.weight False\n",
      "bert.encoder.layer.3.attention.self.key.bias False\n",
      "bert.encoder.layer.3.attention.self.value.weight False\n",
      "bert.encoder.layer.3.attention.self.value.bias False\n",
      "bert.encoder.layer.3.attention.output.dense.weight False\n",
      "bert.encoder.layer.3.attention.output.dense.bias False\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.3.intermediate.dense.weight False\n",
      "bert.encoder.layer.3.intermediate.dense.bias False\n",
      "bert.encoder.layer.3.output.dense.weight False\n",
      "bert.encoder.layer.3.output.dense.bias False\n",
      "bert.encoder.layer.3.output.LayerNorm.weight False\n",
      "bert.encoder.layer.3.output.LayerNorm.bias False\n",
      "bert.encoder.layer.4.attention.self.query.weight False\n",
      "bert.encoder.layer.4.attention.self.query.bias False\n",
      "bert.encoder.layer.4.attention.self.key.weight False\n",
      "bert.encoder.layer.4.attention.self.key.bias False\n",
      "bert.encoder.layer.4.attention.self.value.weight False\n",
      "bert.encoder.layer.4.attention.self.value.bias False\n",
      "bert.encoder.layer.4.attention.output.dense.weight False\n",
      "bert.encoder.layer.4.attention.output.dense.bias False\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.4.intermediate.dense.weight False\n",
      "bert.encoder.layer.4.intermediate.dense.bias False\n",
      "bert.encoder.layer.4.output.dense.weight False\n",
      "bert.encoder.layer.4.output.dense.bias False\n",
      "bert.encoder.layer.4.output.LayerNorm.weight False\n",
      "bert.encoder.layer.4.output.LayerNorm.bias False\n",
      "bert.encoder.layer.5.attention.self.query.weight False\n",
      "bert.encoder.layer.5.attention.self.query.bias False\n",
      "bert.encoder.layer.5.attention.self.key.weight False\n",
      "bert.encoder.layer.5.attention.self.key.bias False\n",
      "bert.encoder.layer.5.attention.self.value.weight False\n",
      "bert.encoder.layer.5.attention.self.value.bias False\n",
      "bert.encoder.layer.5.attention.output.dense.weight False\n",
      "bert.encoder.layer.5.attention.output.dense.bias False\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.5.intermediate.dense.weight False\n",
      "bert.encoder.layer.5.intermediate.dense.bias False\n",
      "bert.encoder.layer.5.output.dense.weight False\n",
      "bert.encoder.layer.5.output.dense.bias False\n",
      "bert.encoder.layer.5.output.LayerNorm.weight False\n",
      "bert.encoder.layer.5.output.LayerNorm.bias False\n",
      "bert.encoder.layer.6.attention.self.query.weight False\n",
      "bert.encoder.layer.6.attention.self.query.bias False\n",
      "bert.encoder.layer.6.attention.self.key.weight False\n",
      "bert.encoder.layer.6.attention.self.key.bias False\n",
      "bert.encoder.layer.6.attention.self.value.weight False\n",
      "bert.encoder.layer.6.attention.self.value.bias False\n",
      "bert.encoder.layer.6.attention.output.dense.weight False\n",
      "bert.encoder.layer.6.attention.output.dense.bias False\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.6.intermediate.dense.weight False\n",
      "bert.encoder.layer.6.intermediate.dense.bias False\n",
      "bert.encoder.layer.6.output.dense.weight False\n",
      "bert.encoder.layer.6.output.dense.bias False\n",
      "bert.encoder.layer.6.output.LayerNorm.weight False\n",
      "bert.encoder.layer.6.output.LayerNorm.bias False\n",
      "bert.encoder.layer.7.attention.self.query.weight False\n",
      "bert.encoder.layer.7.attention.self.query.bias False\n",
      "bert.encoder.layer.7.attention.self.key.weight False\n",
      "bert.encoder.layer.7.attention.self.key.bias False\n",
      "bert.encoder.layer.7.attention.self.value.weight False\n",
      "bert.encoder.layer.7.attention.self.value.bias False\n",
      "bert.encoder.layer.7.attention.output.dense.weight False\n",
      "bert.encoder.layer.7.attention.output.dense.bias False\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.7.intermediate.dense.weight False\n",
      "bert.encoder.layer.7.intermediate.dense.bias False\n",
      "bert.encoder.layer.7.output.dense.weight False\n",
      "bert.encoder.layer.7.output.dense.bias False\n",
      "bert.encoder.layer.7.output.LayerNorm.weight False\n",
      "bert.encoder.layer.7.output.LayerNorm.bias False\n",
      "bert.encoder.layer.8.attention.self.query.weight False\n",
      "bert.encoder.layer.8.attention.self.query.bias False\n",
      "bert.encoder.layer.8.attention.self.key.weight False\n",
      "bert.encoder.layer.8.attention.self.key.bias False\n",
      "bert.encoder.layer.8.attention.self.value.weight False\n",
      "bert.encoder.layer.8.attention.self.value.bias False\n",
      "bert.encoder.layer.8.attention.output.dense.weight False\n",
      "bert.encoder.layer.8.attention.output.dense.bias False\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.8.intermediate.dense.weight False\n",
      "bert.encoder.layer.8.intermediate.dense.bias False\n",
      "bert.encoder.layer.8.output.dense.weight False\n",
      "bert.encoder.layer.8.output.dense.bias False\n",
      "bert.encoder.layer.8.output.LayerNorm.weight False\n",
      "bert.encoder.layer.8.output.LayerNorm.bias False\n",
      "bert.encoder.layer.9.attention.self.query.weight False\n",
      "bert.encoder.layer.9.attention.self.query.bias False\n",
      "bert.encoder.layer.9.attention.self.key.weight False\n",
      "bert.encoder.layer.9.attention.self.key.bias False\n",
      "bert.encoder.layer.9.attention.self.value.weight False\n",
      "bert.encoder.layer.9.attention.self.value.bias False\n",
      "bert.encoder.layer.9.attention.output.dense.weight False\n",
      "bert.encoder.layer.9.attention.output.dense.bias False\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.9.intermediate.dense.weight False\n",
      "bert.encoder.layer.9.intermediate.dense.bias False\n",
      "bert.encoder.layer.9.output.dense.weight False\n",
      "bert.encoder.layer.9.output.dense.bias False\n",
      "bert.encoder.layer.9.output.LayerNorm.weight False\n",
      "bert.encoder.layer.9.output.LayerNorm.bias False\n",
      "bert.encoder.layer.10.attention.self.query.weight False\n",
      "bert.encoder.layer.10.attention.self.query.bias False\n",
      "bert.encoder.layer.10.attention.self.key.weight False\n",
      "bert.encoder.layer.10.attention.self.key.bias False\n",
      "bert.encoder.layer.10.attention.self.value.weight False\n",
      "bert.encoder.layer.10.attention.self.value.bias False\n",
      "bert.encoder.layer.10.attention.output.dense.weight False\n",
      "bert.encoder.layer.10.attention.output.dense.bias False\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.10.intermediate.dense.weight False\n",
      "bert.encoder.layer.10.intermediate.dense.bias False\n",
      "bert.encoder.layer.10.output.dense.weight False\n",
      "bert.encoder.layer.10.output.dense.bias False\n",
      "bert.encoder.layer.10.output.LayerNorm.weight False\n",
      "bert.encoder.layer.10.output.LayerNorm.bias False\n",
      "bert.encoder.layer.11.attention.self.query.weight False\n",
      "bert.encoder.layer.11.attention.self.query.bias False\n",
      "bert.encoder.layer.11.attention.self.key.weight False\n",
      "bert.encoder.layer.11.attention.self.key.bias False\n",
      "bert.encoder.layer.11.attention.self.value.weight False\n",
      "bert.encoder.layer.11.attention.self.value.bias False\n",
      "bert.encoder.layer.11.attention.output.dense.weight False\n",
      "bert.encoder.layer.11.attention.output.dense.bias False\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.11.intermediate.dense.weight False\n",
      "bert.encoder.layer.11.intermediate.dense.bias False\n",
      "bert.encoder.layer.11.output.dense.weight False\n",
      "bert.encoder.layer.11.output.dense.bias False\n",
      "bert.encoder.layer.11.output.LayerNorm.weight False\n",
      "bert.encoder.layer.11.output.LayerNorm.bias False\n",
      "bert.pooler.dense.weight True\n",
      "bert.pooler.dense.bias True\n",
      "classifier.weight True\n",
      "classifier.bias True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "   print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c041464-01f7-4578-8152-67268717463b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 16000/16000 [00:06<00:00, 2429.75 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 2382.68 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 2237.31 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def preprocess_and_tokenize(examples):\n",
    "    preprocessed_texts = []\n",
    "    for text in examples[\"text\"]:\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"\\d+\", \"\", text)\n",
    "        text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "        text = text.strip()\n",
    "        preprocessed_texts.append(text)\n",
    "        \n",
    "    encoding = tokenizer(preprocessed_texts, padding=\"max_length\", truncation=True)\n",
    "    encoding[\"labels\"] = [int(label) for label in examples[\"labels\"]]\n",
    "    return encoding\n",
    "\n",
    "tokenized_data = dataset_dict.map(preprocess_and_tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a67d3f22-852c-4e3b-b71c-e0ff31d0a690",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "auc_score = evaluate.load(\"roc_auc\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    \n",
    "    probabilities = np.exp(predictions) / np.exp(predictions).sum(-1, keepdims=True)\n",
    "    \n",
    "    positive_class_probs = probabilities[:, 1]\n",
    "    \n",
    "    auc = np.round(auc_score.compute(prediction_scores=positive_class_probs, references=labels)['roc_auc'],3)\n",
    "    \n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    acc = np.round(accuracy.compute(predictions=predicted_classes, references=labels)['accuracy'],3)\n",
    "    \n",
    "    return {\"Accuracy\": acc, \"AUC\": auc}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1777e5cc-4018-4abf-a5af-871787c0bd75",
   "metadata": {},
   "source": [
    "Training Using CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0987dc5-e0c9-4b26-881d-94d5762d5d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "        batch = {k: torch.tensor([d[k] for d in batch]) for k in batch[0]}\n",
    "        batch[\"labels\"] = batch[\"labels\"].long()\n",
    "        # print(batch)\n",
    "        return batch\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd61c690-514c-4e89-a296-c9ac91a82ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3050 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60933c1a-0b0c-4fdb-82c1-33baa48ea298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 2e-4\n",
    "batch_size = 50\n",
    "num_epochs = 5\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"bert-ai-classifier_teacher\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    logging_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21693f8c-38eb-4cbf-a5c4-6b9f24df2aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jai54\\AppData\\Local\\Temp\\ipykernel_15872\\1966093435.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1600' max='1600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1600/1600 1:03:47, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.223700</td>\n",
       "      <td>0.170985</td>\n",
       "      <td>0.932000</td>\n",
       "      <td>0.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.141500</td>\n",
       "      <td>0.103810</td>\n",
       "      <td>0.966000</td>\n",
       "      <td>0.993000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.128900</td>\n",
       "      <td>0.180976</td>\n",
       "      <td>0.922000</td>\n",
       "      <td>0.994000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.123600</td>\n",
       "      <td>0.133107</td>\n",
       "      <td>0.948000</td>\n",
       "      <td>0.994000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.119000</td>\n",
       "      <td>0.120313</td>\n",
       "      <td>0.956000</td>\n",
       "      <td>0.995000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1600, training_loss=0.14733776092529297, metrics={'train_runtime': 3828.9021, 'train_samples_per_second': 20.894, 'train_steps_per_second': 0.418, 'total_flos': 2.10488844288e+16, 'train_loss': 0.14733776092529297, 'epoch': 5.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model.to(\"cuda\"),\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37260f6b-2ca1-4c1e-846d-e0e22a2c1670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': np.float64(0.959), 'AUC': np.float64(0.992)}\n"
     ]
    }
   ],
   "source": [
    "# apply model to validation dataset\n",
    "predictions = trainer.predict(tokenized_data[\"train\"])\n",
    "\n",
    "logits = predictions.predictions\n",
    "labels = predictions.label_ids\n",
    "\n",
    "metrics = compute_metrics((logits, labels))\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b045b444-ab45-41f7-a2db-3d79703630bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model.eval()\n",
    "def aipred(sentence):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    \n",
    "    inputs = tokenizer(sentence, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    inputs = {key: val.to(\"cuda\") for key, val in inputs.items()}  # Move tensors to CUDA\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    predicted_label = torch.argmax(probs, dim=-1).item()\n",
    "    \n",
    "    print(f\"{((probs.data[0][1])*100):.2f}% AI Generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "821143ad-47b8-494e-a69a-db040e3bb995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.42% AI Generated\n"
     ]
    }
   ],
   "source": [
    "aipred(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c62c19-3ee1-4159-a44e-873892e9c64e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
